\documentclass[12pt,letterpaper, onecolumn]{exam}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{commath}
\usepackage{physics}
\usepackage{multirow}
\usepackage{float}
\usepackage{relsize}
\usepackage{tikz}
\usepackage[lmargin=71pt, tmargin=1.2in]{geometry}  %For centering solution box
\usepackage{clrscode}
\usepackage{listings}
\usepackage{xcolor}
\usepackage{pdfpages}
\usepackage{enumitem}
\definecolor{codegreen}{rgb}{0,0.6,0}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{codepurple}{rgb}{0.58,0,0.82}
\definecolor{backcolour}{rgb}{0.95,0.95,0.92}
\usetikzlibrary{arrows.meta}
\usetikzlibrary{patterns}
\graphicspath{ {./images/} }

\lstdefinestyle{mystyle}{
    backgroundcolor=\color{backcolour},   
    commentstyle=\color{codegreen},
    keywordstyle=\color{magenta},
    numberstyle=\tiny\color{codegray},
    stringstyle=\color{codepurple},
    basicstyle=\ttfamily\footnotesize,
    breakatwhitespace=false,         
    breaklines=true,                 
    captionpos=b,                    
    keepspaces=true,                 
    numbers=left,                    
    numbersep=1pt,                  
    showspaces=false,                
    showstringspaces=false,
    showtabs=false,                  
    tabsize=2
}

%\lstset{style=mystyle}
\lhead{CAP 5610 Assignment \#3 Solution\\}
\rhead{Arman Sayan\\}
% \chead{\hline} % Un-comment to draw line below header
\thispagestyle{empty}   %For removing header/footer from page 1

\begin{document}

\begingroup  
    \centering
    \LARGE CAP 5610\\
    \LARGE Assignment \#3 Solution\\[0.5em]
    \large \today\\[0.5em]
    \large Arman Sayan\par
\endgroup
\rule{\textwidth}{0.4pt}
\bracketedpoints   %Self-explanatory
\printanswers
\renewcommand{\solutiontitle}{\noindent\textbf{Ans:}\enspace}   %Replace "Ans:" with starting keyword in solution box
\qformat{\large \textbf{\thequestion \quad \thequestiontitle \quad [\thepoints] \hfill}}
\renewcommand{\thepartno}{\arabic{partno}}
\renewcommand{\partlabel}{\thepartno.}

\begin{questions}
    \titledquestion{Kernel Computation Cost}[30]
    
    \begin{parts}
        \part[10]  Consider we have a two-dimensional input space such that the input vector is
        $x = (x_1, x_2)^T$. Define the feature mapping $\phi(x) = (x_1^2, \sqrt{2}x_1x_2, x_2^2)^T$. What is the corresponding
        kernel function, i.e., $K(x, z)$? Do not leave $\phi(x)$ in your final answer.

        \begin{solution}

            To find the kernel function $K(x, z)$, we use the definition of the kernel function as follows:

            \begin{equation*}
                K(x, z) = \phi(x)^T \phi(z)
            \end{equation*}

            where $\phi(x)$ represents feature mapping.

            Given that $\phi(x) = (x_1^2, \sqrt{2}x_1x_2, x_2^2)^T$, for another input vector $z = (z_1, z_2)^T$,
            the feature mapping is $\phi(z) = (z_1^2, \sqrt{2}z_1z_2, z_2^2)^T$.

            Now, we can compute the dot product of the feature mappings as follows:

            \begin{align*}
                K(x, z) &= \phi(x)^T \phi(z)\\ 
                &= \begin{bmatrix}
                    x_1^2 & \sqrt{2}x_1x_2 & x_2^2
                \end{bmatrix}
                \begin{bmatrix}
                    z_1^2\\
                    \sqrt{2}z_1z_2\\
                    z_2^2
                \end{bmatrix}\\
                &= x_1^2z_1^2 + 2x_1x_2z_1z_2 + x_2^2z_2^2\\
            \end{align*}

            With a deduction from the above calculations, we can observe the final expression as

            \begin{center}
                $K(x, z) = (x_1z_1 + x_2z_2)^2$
            \end{center}

            Since $x_1z_1 + x_2z_2$ is the dot product of the input vectors $x$ and $z$, namely $x^Tz$, 
            the corresponding kernel function is 

            \begin{center}
                \fbox{$\displaystyle{\boldsymbol{K(x, z) = (x^Tz)^2}}$}
            \end{center}

        \end{solution}

        \part[20] Suppose we want to compute the value of the kernel function $K(x, z)$ from the
        previous question on two vectors $x, z \in \mathbb{R}^2$. How many additions and multiplications are
        needed if you

        \begin{subparts}
            
            \subpart[10] Map the input vector to the feature space and then perform the dot product
            on the mapped features?

            \begin{solution}

                Using the feature mappings 
                
                \begin{center}
                    $\phi(x) = (x_1^2, \sqrt{2}x_1x_2, x_2^2)^T$ and $\phi(z) = (z_1^2, \sqrt{2}z_1z_2, z_2^2)^T$,
                \end{center}
                
                the kernel function $K(x, z)$ can be computed as

                \begin{align*}
                    K(x, z) &= \phi(x)^T \phi(z)\\ 
                    &= \begin{bmatrix}
                        x_1^2 & \sqrt{2}x_1x_2 & x_2^2
                    \end{bmatrix}
                    \begin{bmatrix}
                        z_1^2\\
                        \sqrt{2}z_1z_2\\
                        z_2^2
                    \end{bmatrix}\\
                    &= x_1^2z_1^2 + 2x_1x_2z_1z_2 + x_2^2z_2^2\\
                \end{align*}

                For the feature mapping of the vector $x$, we need to compute terms $x_1^2$, $x_2^2$, and $\sqrt{2}x_1x_2$.
                To calculate these terms, we need 3 multiplications.

                Since we have 2 input vectors $x$ and $z$, we need to perform 3 multiplications for each vector, which results in 6 multiplications
                for mapping the vectors to the feature space.

                After mapping the vectors to the feature space, we need to compute the dot product of the feature mappings as

                \begin{center}
                    $K(x, z) = x_1^2z_1^2 + 2x_1x_2z_1z_2 + x_2^2z_2^2$
                \end{center}

                This requires 3 multiplications for each term in the summation and 2 additions to sum three terms. 
                
                In total, we need \fbox{\textbf{9 multiplications and 2 additions}} to compute the value of $K(x, z)$.

            \end{solution}

            \pagebreak

            \subpart[10] Compute through the kernel function you derived in question 1?

            \begin{solution}

                The kernel function derived in question 1 is $K(x, z) = (x^Tz)^2$.

                To compute the value of the kernel function $K(x, z)$ using this kernel function, we need to calculate the dot product of the input vectors $x$ and $z$,
                namely $x^Tz$.

                The dot product of two vectors $x$ and $z$ is calculated as

                \begin{center}
                    $x^Tz = x_1z_1 + x_2z_2$
                \end{center}

                This requires 2 multiplications for terms $x_1z_1$ and $x_2z_2$ and 1 addition to sum those terms.

                After computing the dot product, we need to square the result to obtain the value of the kernel function $K(x, z)$.

                Squaring the result requires 1 multiplication.

                In total, we need \fbox{\textbf{3 multiplications and 1 addition}} to compute the value of $K(x, z)$.
                
            \end{solution}

        \end{subparts}

    \end{parts}

    \pagebreak

    \titledquestion{Activation Functions and Loss Functions}[30]

    \begin{parts}

        \part[20] For this assignment, you are encouraged to consult Dr. GOOGLE and Dr. ChatGPT. But please explain
        things in your own language while you write the answer. For each of the following activation functions,
        briefly describe the type of non-linearity (if any) it introduces and discuss their pros and cons.

        \begin{enumerate}[label=(\alph*)]
            \item Linear Activation Function
            \item Sigmoid Activation Function
            \item Tanh Activation Function
            \item ReLU (Rectified Linear Unit) Activation Function
        \end{enumerate}

        \begin{solution}

        \end{solution}

        \pagebreak

        \part[10] For each of the following loss functions, briefly describe their mathematical formula and discuss for which type of
        learning task (classification, regression, etc.) they are most appropriate.

        \begin{enumerate}[label=(\alph*)]
            \item Mean Squared Error Loss
            \item Binary Cross Entropy Loss
            \item Hinge Loss
            \item Softmax Cross Entropy Loss
        \end{enumerate}

        \begin{solution}

        \end{solution}
        
    \end{parts}

    \pagebreak

    \titledquestion{Linear SVM Implementation}[40]

    Solution for Q3:

    \begin{solution}

        Please check the source code and outputs included in the appendix named as

        \begin{center}
            \textbf{CAP\_5610\_Assignment\_3\_Solution\_Arman\_Sayan.ipynb}
        \end{center}
        
        for the solution.
    \end{solution}

    \pagebreak
    
\end{questions}

%\begin{appendix}
%    \centering
%    \begin{flushleft}  
%      \section{Appendix}
%      \includepdf[pages=-]{CAP_5610_Assignment_3_Solution_Arman_Sayan_1.pdf}
%    \end{flushleft}
%\end{appendix}

\end{document}